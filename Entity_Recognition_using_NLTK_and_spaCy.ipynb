{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUgmfClFTTYQHXWuzDD5Sh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/griisnc/PLN_Python/blob/main/Entity_Recognition_using_NLTK_and_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code was made by Griselda Navarrete"
      ],
      "metadata": {
        "id": "6ZMvXiwSeODR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description:"
      ],
      "metadata": {
        "id": "NrpS1M0IaBtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs Natural Language Processing (NLP) on a Spanish text file. It first mounts Google Drive to access the file, then uses the NLTK library to tokenize the text and remove stop words. It further cleans the tokens by removing punctuation and special characters. After that, it uses the spaCy library to identify named entities within the text, printing the entity and its corresponding label.\n",
        "\n",
        "In essence, this code prepares a Spanish text file for analysis by tokenizing, cleaning, and identifying key named entities within it using popular NLP libraries like NLTK and spaCy."
      ],
      "metadata": {
        "id": "cd8j72X2aEkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NKWJFiInvZGR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from IPython import get_ipython # Imports the get_ipython function from the IPython module, which is used to interact with the current IPython session.\n",
        "from IPython.display import display # Imports the display function from the IPython.display module, which is used to display objects in the output area of a notebook cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Imports the os module, which provides functions for interacting with the operating system, such as file and directory manipulation.\n",
        "import nltk # Imports the nltk (Natural Language Toolkit) library, which is used for natural language processing tasks.\n",
        "import random # Imports the random module, which provides functions for generating random numbers and making random choices.\n"
      ],
      "metadata": {
        "id": "R6REyNg9cztw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Imports the drive module from the google.colab library, which is used to mount Google Drive in a Colab notebook.\n",
        "drive.mount('/content/drive') # Mounts Google Drive to the '/content/drive' directory in the Colab environment. This allows access to files stored in your Google Drive."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2cEgSq5v4z3",
        "outputId": "88c6a95a-1192-42ed-b1ab-1ef000d05251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') # Downloads the 'punkt_tab' resource from NLTK, which is used for tokenizing text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Ypk00-wPtP",
        "outputId": "d68551c4-3a40-4adf-9694-8f5f7bf1172b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archivo =random.sample(os.listdir('/content/drive/MyDrive/ia2_3p/'),1)\n",
        "archivo = archivo[0]\n",
        "archivo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lm0-EjWMwVYn",
        "outputId": "72c3e52f-f50f-4700-dd45-2d7b66683076"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'resenia.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ia2_3p/'+archivo,\"r\", encoding=\"utf8\") as entrada: # Opens the selected file in read mode ('r') with UTF-8 encoding and assigns it to the variable 'entrada'.\n",
        "    texto = entrada.read() # Reads the content of the file and assigns it to the variable 'texto'."
      ],
      "metadata": {
        "id": "dZnLyHfZw38R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(texto,\"spanish\") # Tokenizes the text using NLTK's word_tokenize function, specifying Spanish as the language.\n",
        "tokens # Prints the tokens."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0QP3Xhqxr6g",
        "outputId": "24e28337-e655-4906-b931-249b6c47efa2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mark',\n",
              " 'Zuckerberg',\n",
              " ',',\n",
              " 'uno',\n",
              " 'de',\n",
              " 'los',\n",
              " 'fundadores',\n",
              " 'de',\n",
              " 'Facebook',\n",
              " 'y',\n",
              " 'presidente',\n",
              " 'de',\n",
              " 'Meta',\n",
              " ',',\n",
              " 'se',\n",
              " 'reunió',\n",
              " 'este',\n",
              " 'miércoles',\n",
              " 'con',\n",
              " 'el',\n",
              " 'presidente',\n",
              " 'electo',\n",
              " 'de',\n",
              " 'Estados',\n",
              " 'Unidos',\n",
              " ',',\n",
              " 'Donald',\n",
              " 'Trump',\n",
              " ',',\n",
              " 'en',\n",
              " 'la',\n",
              " 'residencia',\n",
              " 'de',\n",
              " 'este',\n",
              " 'último',\n",
              " 'en',\n",
              " 'Mar-a-Lago',\n",
              " '(',\n",
              " 'Florida',\n",
              " ')',\n",
              " ',',\n",
              " 'según',\n",
              " 'informaron',\n",
              " 'los',\n",
              " 'diarios',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " 'y',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Post',\n",
              " '.',\n",
              " 'De',\n",
              " 'acuerdo',\n",
              " 'con',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " ',',\n",
              " 'que',\n",
              " 'cita',\n",
              " 'a',\n",
              " 'tres',\n",
              " 'personas',\n",
              " 'con',\n",
              " 'conocimiento',\n",
              " 'del',\n",
              " 'encuentro',\n",
              " ',',\n",
              " 'Zuckerberg',\n",
              " 'buscó',\n",
              " 'la',\n",
              " 'reunión',\n",
              " 'con',\n",
              " 'Trump',\n",
              " 'como',\n",
              " 'un',\n",
              " 'intento',\n",
              " 'de',\n",
              " 'mejorar',\n",
              " 'la',\n",
              " 'relación',\n",
              " 'entre',\n",
              " 'ambos',\n",
              " 'tras',\n",
              " 'una',\n",
              " 'década',\n",
              " 'marcada',\n",
              " 'por',\n",
              " 'tensiones',\n",
              " '.',\n",
              " 'Trump',\n",
              " 'ha',\n",
              " 'acusado',\n",
              " 'en',\n",
              " 'repetidas',\n",
              " 'ocasiones',\n",
              " 'a',\n",
              " 'Meta',\n",
              " 'de',\n",
              " 'censurar',\n",
              " 'injustamente',\n",
              " 'sus',\n",
              " 'opiniones',\n",
              " 'y',\n",
              " 'las',\n",
              " 'de',\n",
              " 'otras',\n",
              " 'voces',\n",
              " 'conservadoras',\n",
              " 'en',\n",
              " 'sus',\n",
              " 'plataformas',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords # Imports the stopwords module from NLTK's corpus package, which contains lists of common stop words for various languages."
      ],
      "metadata": {
        "id": "g54I15R8xwgA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB4f4iAZxy_8",
        "outputId": "9daeff95-d6d3-4f11-d3f4-5c44fe29f5b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') # Downloads the 'stopwords' resource from NLTK, which contains lists of stop words for various languages.\n"
      ],
      "metadata": {
        "id": "u1WP3LKux2q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a73a91-a320-4241-e41d-c6be552f7484"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('spanish')) # Creates a set of Spanish stop words using NLTK's stopwords corpus.\n"
      ],
      "metadata": {
        "id": "8EbP6NZPdcG6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,token in enumerate(tokens): # Iterates through the tokens list, using enumerate to get both the index (i) and the token value.\n",
        "    if token.startswith(\"_\") or token.startswith(\"—\"): # Checks if the token starts with an underscore or a dash.\n",
        "        tokens[i] = tokens[i][1:] # If it does, removes the first character of the token.\n",
        "    if token.endswith(\"_\") or token.endswith(\"—\"): # Checks if the token ends with an underscore or a dash.\n",
        "        tokens[i] = tokens[i][:-1] # If it does, removes the last character of the token.\n",
        "texto = \" \".join(tokens) # Joins the tokens back into a string, separated by spaces.\n"
      ],
      "metadata": {
        "id": "DHQ6n2cyyEwo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar(texto): # Defines a function called 'tokenizar' that takes a text string as input.\n",
        "    puntuacion = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¿¡' # Defines a string containing punctuation characters.\n",
        "    tokens = nltk.word_tokenize(texto,\"spanish\") # Tokenizes the input text using NLTK's word_tokenize function, specifying Spanish as the language.\n",
        "    for i,token in enumerate(tokens): # Iterates through the tokens list, using enumerate to get both the index (i) and the token value.\n",
        "        tokens[i] = token.strip(puntuacion) # Removes any punctuation characters from the beginning and end of each token using the strip method.\n",
        "    texto = \" \".join(tokens) # Joins the tokens back into a string, separated by spaces.\n",
        "    tokens = nltk.word_tokenize(texto,\"spanish\") # Tokenizes the text again after removing punctuation.\n",
        "    return tokens # Returns the final list of tokens."
      ],
      "metadata": {
        "id": "dwARWITNv6WH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizar(texto)\n",
        "tokens # Prints the tokens."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huqQiLZUyS5F",
        "outputId": "c4355229-8b9d-4635-bb09-4130308c5703"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mark',\n",
              " 'Zuckerberg',\n",
              " 'uno',\n",
              " 'de',\n",
              " 'los',\n",
              " 'fundadores',\n",
              " 'de',\n",
              " 'Facebook',\n",
              " 'y',\n",
              " 'presidente',\n",
              " 'de',\n",
              " 'Meta',\n",
              " 'se',\n",
              " 'reunió',\n",
              " 'este',\n",
              " 'miércoles',\n",
              " 'con',\n",
              " 'el',\n",
              " 'presidente',\n",
              " 'electo',\n",
              " 'de',\n",
              " 'Estados',\n",
              " 'Unidos',\n",
              " 'Donald',\n",
              " 'Trump',\n",
              " 'en',\n",
              " 'la',\n",
              " 'residencia',\n",
              " 'de',\n",
              " 'este',\n",
              " 'último',\n",
              " 'en',\n",
              " 'Mar-a-Lago',\n",
              " 'Florida',\n",
              " 'según',\n",
              " 'informaron',\n",
              " 'los',\n",
              " 'diarios',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " 'y',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Post',\n",
              " 'De',\n",
              " 'acuerdo',\n",
              " 'con',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " 'que',\n",
              " 'cita',\n",
              " 'a',\n",
              " 'tres',\n",
              " 'personas',\n",
              " 'con',\n",
              " 'conocimiento',\n",
              " 'del',\n",
              " 'encuentro',\n",
              " 'Zuckerberg',\n",
              " 'buscó',\n",
              " 'la',\n",
              " 'reunión',\n",
              " 'con',\n",
              " 'Trump',\n",
              " 'como',\n",
              " 'un',\n",
              " 'intento',\n",
              " 'de',\n",
              " 'mejorar',\n",
              " 'la',\n",
              " 'relación',\n",
              " 'entre',\n",
              " 'ambos',\n",
              " 'tras',\n",
              " 'una',\n",
              " 'década',\n",
              " 'marcada',\n",
              " 'por',\n",
              " 'tensiones',\n",
              " 'Trump',\n",
              " 'ha',\n",
              " 'acusado',\n",
              " 'en',\n",
              " 'repetidas',\n",
              " 'ocasiones',\n",
              " 'a',\n",
              " 'Meta',\n",
              " 'de',\n",
              " 'censurar',\n",
              " 'injustamente',\n",
              " 'sus',\n",
              " 'opiniones',\n",
              " 'y',\n",
              " 'las',\n",
              " 'de',\n",
              " 'otras',\n",
              " 'voces',\n",
              " 'conservadoras',\n",
              " 'en',\n",
              " 'sus',\n",
              " 'plataformas']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = [word for word in tokens if word.isalpha()] # Creates a new list called 'word_tokens' containing only the tokens that are alphabetic. This is done using a list comprehension.\n"
      ],
      "metadata": {
        "id": "93d-679VydI0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filteres_sentence =  [w for w in word_tokens if not w.lower() in stop_words] # Creates a new list called 'filteres_sentence' containing only the word tokens that are not in the stop_words set. This is done using a list comprehension and converts the word tokens to lowercase before checking if they are in the stop_words set."
      ],
      "metadata": {
        "id": "obT59kdqyeKV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [] # Initializes an empty list called 'filtered_sentence'."
      ],
      "metadata": {
        "id": "R-jDSplHyjt0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in word_tokens: # Iterates through the word_tokens list.\n",
        "    if w not in stop_words: # Checks if the current word token is not in the stop_words set.\n",
        "        filtered_sentence.append(w) # If it is not a stop word, appends it to the filtered_sentence list.\n",
        "print(word_tokens) # Prints the word_tokens list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUVwCGvUymKz",
        "outputId": "ca13a090-099d-403d-d147-423d592f3af8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mark', 'Zuckerberg', 'uno', 'de', 'los', 'fundadores', 'de', 'Facebook', 'y', 'presidente', 'de', 'Meta', 'se', 'reunió', 'este', 'miércoles', 'con', 'el', 'presidente', 'electo', 'de', 'Estados', 'Unidos', 'Donald', 'Trump', 'en', 'la', 'residencia', 'de', 'este', 'último', 'en', 'Florida', 'según', 'informaron', 'los', 'diarios', 'The', 'New', 'York', 'Times', 'y', 'The', 'New', 'York', 'Post', 'De', 'acuerdo', 'con', 'The', 'New', 'York', 'Times', 'que', 'cita', 'a', 'tres', 'personas', 'con', 'conocimiento', 'del', 'encuentro', 'Zuckerberg', 'buscó', 'la', 'reunión', 'con', 'Trump', 'como', 'un', 'intento', 'de', 'mejorar', 'la', 'relación', 'entre', 'ambos', 'tras', 'una', 'década', 'marcada', 'por', 'tensiones', 'Trump', 'ha', 'acusado', 'en', 'repetidas', 'ocasiones', 'a', 'Meta', 'de', 'censurar', 'injustamente', 'sus', 'opiniones', 'y', 'las', 'de', 'otras', 'voces', 'conservadoras', 'en', 'sus', 'plataformas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence # Prints the filtered_sentence list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgySBhwLyxjr",
        "outputId": "e950e6b5-6415-44db-ddb2-a4515a7420ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mark',\n",
              " 'Zuckerberg',\n",
              " 'fundadores',\n",
              " 'Facebook',\n",
              " 'presidente',\n",
              " 'Meta',\n",
              " 'reunió',\n",
              " 'miércoles',\n",
              " 'presidente',\n",
              " 'electo',\n",
              " 'Estados',\n",
              " 'Unidos',\n",
              " 'Donald',\n",
              " 'Trump',\n",
              " 'residencia',\n",
              " 'último',\n",
              " 'Florida',\n",
              " 'según',\n",
              " 'informaron',\n",
              " 'diarios',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Post',\n",
              " 'De',\n",
              " 'acuerdo',\n",
              " 'The',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Times',\n",
              " 'cita',\n",
              " 'tres',\n",
              " 'personas',\n",
              " 'conocimiento',\n",
              " 'encuentro',\n",
              " 'Zuckerberg',\n",
              " 'buscó',\n",
              " 'reunión',\n",
              " 'Trump',\n",
              " 'intento',\n",
              " 'mejorar',\n",
              " 'relación',\n",
              " 'ambos',\n",
              " 'tras',\n",
              " 'década',\n",
              " 'marcada',\n",
              " 'tensiones',\n",
              " 'Trump',\n",
              " 'acusado',\n",
              " 'repetidas',\n",
              " 'ocasiones',\n",
              " 'Meta',\n",
              " 'censurar',\n",
              " 'injustamente',\n",
              " 'opiniones',\n",
              " 'voces',\n",
              " 'conservadoras',\n",
              " 'plataformas']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spaCy # Installs the spaCy library using pip. This library is used for natural language processing tasks.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmeWDTlqz6kM",
        "outputId": "9fa8ae28-a57e-41da-8c74-987f89f50024"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spaCy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spaCy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spaCy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm # Downloads the Spanish language model for spaCy called 'es_core_news_sm'. This model is required for processing Spanish text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTah7B150BwT",
        "outputId": "8c87df68-eab0-4815-b180-0c5f02dcd928"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # Imports the spaCy library.\n",
        "\n",
        "# Load the Spanish language model\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(\" \".join(filtered_sentence))\n",
        "\n",
        "# Print the identified entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDHb4p210Z17",
        "outputId": "ae25027a-264d-49dc-b84c-33c148b56b13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Mark Zuckerberg, Label: PER\n",
            "Entity: Facebook presidente Meta, Label: MISC\n",
            "Entity: Estados Unidos, Label: LOC\n",
            "Entity: Donald Trump, Label: PER\n",
            "Entity: Florida, Label: LOC\n",
            "Entity: The New York Times The New York Post, Label: ORG\n",
            "Entity: The New York Times, Label: ORG\n",
            "Entity: Zuckerberg, Label: PER\n",
            "Entity: Trump, Label: LOC\n",
            "Entity: Trump, Label: LOC\n",
            "Entity: Meta, Label: PER\n"
          ]
        }
      ]
    }
  ]
}